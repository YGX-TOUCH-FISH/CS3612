{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "349DCCCB343F4B649A23AFB0E1822FAA",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Logistic Regression\n",
    "\n",
    "逻辑回归模型是一种常用的分类模型，它通过sigmoid或者softmax函数，将函数值映射到(0, 1)区间内，从而实现对样本的分类。在这个小作业中，你需要实现：\n",
    "1. 二分类和多分类两种逻辑回归模型\n",
    "2. 分别含有 L1 和 L2 两种正则项的损失函数，并计算对应的梯度\n",
    "3. 权重参数W的更新\n",
    "4. 比较不同的学习率对损失函数和分类器性能的影响\n",
    "5. 比较不同的正则项参数对于分类器性能的影响\n",
    "   \n",
    "**本次小作业截止时间：2022/4/4 23:59, 请在canvas系统中及时提交。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81849DCA00F1484AA4EFCB7C8BA3CBDC",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 一、二分类逻辑回归：\n",
    "### 1.1数据集介绍\n",
    "这个任务中使用的数据集是手写数字集MNIST，它有60000个训练样本和10000个测试样本，共10个类别。在二分类任务上，我们对MNIST数据集进行了一个采样，抽取了数据集中的‘5’和‘3’对应的样本作为二分类的正负样本，共得到11552个训练样本，1902个测试样本，其中正负样本数量均相同。为了让大家对于这个数据集有一个更直观的认识，我们从正负样本中各抽取了8个样例进行了可视化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ED97409753E411AAE7237C565E7E95A",
    "jupyter": {},
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def load_data1(path):\n",
    "    # load all MNIST data\n",
    "    fd = open(os.path.join(path, 'train-images.idx3-ubyte'))\n",
    "    loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
    "    train_X_all = loaded[16:].reshape((60000, 28, 28, 1)).astype(np.float)\n",
    "    fd = open(os.path.join(path, 'train-labels.idx1-ubyte'))\n",
    "    loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
    "    train_Y_all = loaded[8:].reshape(60000).astype(np.float)\n",
    "    fd = open(os.path.join(path, 't10k-images.idx3-ubyte'))\n",
    "    loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
    "    test_X_all = loaded[16:].reshape((10000, 28, 28, 1)).astype(np.float)\n",
    "    fd = open(os.path.join(path, 't10k-labels.idx1-ubyte'))\n",
    "    loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
    "    test_Y_all = loaded[8:].reshape(10000).astype(np.float)\n",
    "\n",
    "    #subsample data\n",
    "    train_3_mask, train_5_mask = (train_Y_all == 3), (train_Y_all == 5) \n",
    "    test_3_mask, test_5_mask = (test_Y_all == 3), (test_Y_all == 5)\n",
    "    train_Y_all[train_5_mask] = 1\n",
    "    train_Y_all[train_3_mask] = 0\n",
    "    test_Y_all[test_5_mask] = 1\n",
    "    test_Y_all[test_3_mask] = 0\n",
    "    train_mask = np.logical_or(train_5_mask, train_3_mask)\n",
    "    test_mask = np.logical_or(test_5_mask, test_3_mask)\n",
    "    train_X = train_X_all[train_mask]\n",
    "    train_Y = train_Y_all[train_mask]\n",
    "    test_X = test_X_all[test_mask]\n",
    "    test_Y = test_Y_all[test_mask]\n",
    "\n",
    "    #visualize data\n",
    "    sample_num = 8\n",
    "    for i in range(sample_num):\n",
    "        plt_idx = i + 1\n",
    "        plt.subplot(2, sample_num, plt_idx)\n",
    "        plt.imshow(train_X_all[train_5_mask][i].reshape((28, 28)), cmap=plt.cm.gray)\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title('Positive')\n",
    "\n",
    "    for i in range(sample_num):\n",
    "        plt_idx = sample_num + i + 1\n",
    "        plt.subplot(2, sample_num, plt_idx)\n",
    "        plt.imshow(train_X_all[train_3_mask][i].reshape((28, 28)), cmap=plt.cm.gray)\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title('Negative')\n",
    "    plt.show()\n",
    "    # reshaple into rows and normalize\n",
    "    train_X = train_X.reshape((train_X.shape[0], -1))\n",
    "    test_X = test_X.reshape((test_X.shape[0], -1)) # reshape(shape[0],-1): reshape (a,b,c,d) into (a,b*c*d)\n",
    "    mean_image = np.mean(train_X, axis=0) # axis=0: calculate mean of every column\n",
    "    train_X = train_X - mean_image\n",
    "    test_X = test_X - mean_image\n",
    "\n",
    "    # add a bias column into X\n",
    "    train_X = np.hstack([train_X, np.ones((train_X.shape[0], 1))])\n",
    "    test_X = np.hstack([test_X, np.ones((test_X.shape[0], 1))]) # np.ones/zeros/eyes([row, col])\n",
    "    return train_X, train_Y, test_X, test_Y\n",
    "\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = load_data1('./dataset/Assignment1/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A520F8AD9CEE45198816F39C95819B44",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"X_train shape is {}\".format(X_train.shape))\n",
    "print(\"Y_train shape is {}\".format(Y_train.shape))\n",
    "print(\"X_test shape is {}\".format(X_test.shape))\n",
    "print(\"Y_test shape is {}\".format(Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ADF94CCCE824695A9814D1730001A8D",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 1.2逻辑回归模型\n",
    "在这部分我们将会使用梯度下降算法训练逻辑回归模型。我们将会进行固定次数的迭代，在每个迭代过程中我们将计算数据样本的损失大小，以及对每个权重参数的梯度求导，并在此基础上进行梯度下降。算法的核心公式（此处包括损失函数和梯度下降，未包含正则项，在后面的实际要求中需要实现正则项）如下：\n",
    "\n",
    "$$\n",
    "L(f(x), y) = -y\\log{f(x)}-(1-y)\\log(1-f(x)), f(x) = \\frac{e^{wx + b}}{1 + e^{wx + b}}\n",
    "$$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\forall k, \\frac{\\partial}{\\partial w_k} L(w,b) = \\frac{1}{N}\\sum_{i=1}^N (f(x^{(i)}) - y^{(i)}) x^{(i)}_k\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial b} L(w,b)= \\frac{1}{N}\\sum_{i=1}^N (f(x^{(i)}) - y^{(i)})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "在这一部分中你需要完成以下内容：\n",
    "1. train函数中权重$W$的更新\n",
    "2. L1和L2两种正则化的损失函数及对应梯度的计算\n",
    "3. predict函数中的预测类别的计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11FA5BD34FE841DAB906227A6F51E574",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "603ded5663d3c30015bd3336",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Question0:\n",
    "在下面利用线性模型进行逻辑回归的代码实现中，我们可以只需要权重矩阵 $W$ 而并不需要偏置项 $b$ ，为什么？（提示：之前的数据预处理过程中，除了将 $28 \\times 28$ 的片“拍扁”并normalize以外，我们还做了哪步处理？）\n",
    "\n",
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "C0AAD39A300446338D60432B301DF402",
    "jupyter": {},
    "mdEditEnable": false,
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class LinearRegression1(object):\n",
    "    def __init__(self):\n",
    "        self.W = None\n",
    "    \n",
    "    def train(self, X, Y, learning_rate=1e-3, reg=1e-5, reg_type='L2', show_detail=True, num_iters=2000,\n",
    "          batch_size=128):\n",
    "        num_train, feat_dim = X.shape\n",
    "        self.W = 0.001 * np.random.randn(feat_dim)\n",
    "        loss_history = []\n",
    "        if show_detail:\n",
    "            print(\"train begin\")\n",
    "        for i in range(num_iters):\n",
    "            batch_indices = np.random.choice(num_train, batch_size, replace=True)\n",
    "            X_batch = X[batch_indices]\n",
    "            Y_batch = Y[batch_indices]\n",
    "            if reg_type == 'L1':\n",
    "                loss, grad = self.l1_loss(X_batch, Y_batch, reg)\n",
    "            else:\n",
    "                loss, grad = self.l2_loss(X_batch, Y_batch, reg)\n",
    "            loss_history.append(loss)\n",
    "            \n",
    "            if learning_rate < 0:\n",
    "                learning_rate = learning_rate*((2000-i)/4000+0.5)\n",
    "            ########################################\n",
    "            # TODO: Update W\n",
    "            \n",
    "\n",
    "            ########################################\n",
    "            if show_detail and i % 100 == 0:\n",
    "                print(\"In iteration {}/{} , the loss is {}\".format(i, num_iters, loss))\n",
    "        return loss_history\n",
    "    \n",
    "    def Sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def func_f(self, X):\n",
    "        return 1-self.Sigmoid(-np.dot(X,self.W))\n",
    "\n",
    "    def calc(self, X, Y):\n",
    "        func = self.func_f(X)\n",
    "        loss = np.sum(-Y*np.log(func)-(1-Y)*np.log(1-func))\n",
    "        grad = (np.dot((func-Y).reshape(1,-1), X)).T\n",
    "        grad = np.array(grad).flatten()\n",
    "        return loss / X.shape[0], grad / X.shape[0]\n",
    "\n",
    "    def l1_loss(self, X, Y, reg):\n",
    "        loss = 0\n",
    "        grad = None\n",
    "        \n",
    "        ########################################\n",
    "        # TODO: Calculate the loss and gradient\n",
    "        \n",
    "        \n",
    "        ########################################\n",
    "        \n",
    "        return loss, grad\n",
    "    \n",
    "    def l2_loss(self, X, Y, reg):\n",
    "        loss = 0\n",
    "        grad = None\n",
    "\n",
    "        ########################################\n",
    "        # TODO: Calculate the loss and gradient\n",
    "        \n",
    "        \n",
    "        ########################################\n",
    "\n",
    "        return loss, grad\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        Y_pred = None\n",
    "        \n",
    "        ########################################\n",
    "        # TODO: Predict the label of X with threshold\n",
    "        # Hint: You may use func_f\n",
    "        \n",
    "        ########################################\n",
    "        \n",
    "        return Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C860E4A6D2084E68B890076E8105D4C5",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 1.3 训练模型实例\n",
    "在这一部分，你不需要完成任何代码，你可以通过这一部分验证你上面实现的LogisticRegression1的代码是否实现正确。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8A4053C3BFC742D1A2A0326917E2B4C9",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr_param = 1.5e-6\n",
    "reg_param = 0.01\n",
    "\n",
    "model = LinearRegression1()\n",
    "loss_history = model.train(X_train, Y_train, lr_param, reg_param, 'L2', True)\n",
    "pred = model.predict(X_test)\n",
    "acc = np.mean(pred == Y_test)\n",
    "print(\"The Accuracy is {}\\n\".format(acc))\n",
    "x = range(len(loss_history))\n",
    "plt.plot(x, loss_history, label='Loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Iteration Num')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "W = model.W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27A95F6A3315482A8EF1F4DFE3A0025E",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 1.4 学习率和Loss函数、模型性能的关系\n",
    "因为学习率和正则化参数都是超参数，在一般的训练过程中，我们没办法直接优化，所以我们一般会将训练集细分成训练集和验证集，然后通过模型在验证集上的表现选择一个最优的超参数，再将它对应的最优的模型应用到测试集中。\n",
    "在这一部分你需要完成以下内容：\n",
    "1. 在L1、L2两种正则化下尝试多种不同的学习率\n",
    "2. 储存学习率对应的损失函数值到L1_loss和L2_loss中（我们对损失函数值进行了20步平均化处理）。\n",
    "3. 储存学习率对应的**在验证集上**的正确率到L1_lr_val_acc和L2_lr_val_acc中\n",
    "\n",
    "#### 注意：\n",
    "你可以参考1.3的代码进行训练参数的设置。\n",
    "\n",
    "因为已有代码中L1_loss，L1_lr_val_acc都是数组，在可视化的过程中我们需要学习率和它们相对应，比如learning_rates[0]对应的loss和validation accuracy应该储存在数组index为0的位置\n",
    "\n",
    "#### 拓展：\n",
    "在这个部分中采取的损失函数都是定值，如果你有时间的话，可以尝试根据迭代轮数改变学习率，并比较不变的学习率和变化的学习率对于模型性能的影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "29B73EBF7A1A49A7AB8EF67379778334",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "reg = 0.01\n",
    "reg_types = ['L1', 'L2']\n",
    "L1_loss = []\n",
    "L2_loss = []\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2)\n",
    "L1_lr_val_acc = []\n",
    "L2_lr_val_acc = []\n",
    "\n",
    "########################################\n",
    "# TODO: train & test model with different learning rates\n",
    "\n",
    "\n",
    "\n",
    "########################################\n",
    "#visulize the relationship between lr and loss\n",
    "for i, lr in enumerate(learning_rates):\n",
    "    L1_loss_label = str(lr) + 'L1'\n",
    "    L2_loss_label = str(lr) + 'L2'\n",
    "    L1_loss_i = L1_loss[i]\n",
    "    L2_loss_i = L2_loss[i]\n",
    "    ave_L1_loss = np.zeros_like(L1_loss_i)\n",
    "    ave_L2_loss = np.zeros_like(L2_loss_i)\n",
    "    ave_step = 20\n",
    "    for j in range(len(L1_loss_i)):\n",
    "        if j < ave_step:\n",
    "            ave_L1_loss[j] = np.mean(L1_loss_i[0: j + 1])\n",
    "            ave_L2_loss[j] = np.mean(L2_loss_i[0: j + 1])\n",
    "        else:\n",
    "            ave_L1_loss[j] = np.mean(L1_loss_i[j - ave_step + 1: j + 1])    \n",
    "            ave_L2_loss[j] = np.mean(L2_loss_i[j - ave_step + 1: j + 1])\n",
    "    x = range(len(L1_loss_i))\n",
    "    plt.plot(x, ave_L1_loss, label=L1_loss_label)\n",
    "    plt.plot(x, ave_L2_loss, label=L2_loss_label)\n",
    "    \n",
    "plt.legend()\n",
    "plt.xlabel('high-parameter lr')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "#visulize the relationship between lr and accuracy\n",
    "x = range(len(learning_rates))\n",
    "plt.plot(x, L1_lr_val_acc, label='L1_val_acc')\n",
    "plt.plot(x, L2_lr_val_acc, label='L2_val_acc')\n",
    "plt.xticks(x, learning_rates)\n",
    "plt.margins(0.08)\n",
    "plt.legend()\n",
    "plt.xlabel('high-parameter lr')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DA6289D7D8FF43C1A3257E3FC684E856",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Question1: 学习率和损失函数的变化、模型性能之间分别有什么关系？\n",
    "\n",
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5CBDD6A9235E466482D04B6E7D0C6D5F",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 1.5 正则项与模型性能\n",
    "在这一部分中，你需要完成以下内容：\n",
    "1. 尝试多个正则化参数的值\n",
    "2. 储存对应的在**验证集上**的正确率到L1_reg_val_acc和L2_reg_val_acc中\n",
    "3. 通过验证集X_val和Y_val选择最优的正则化超参数，并储存最优正则化参数和对应模型\n",
    "\n",
    "已有的代码会画出正则化参数和验证集上正确率的关系图，并计算最优的模型在测试集上的正确率。\n",
    "\n",
    "#### 注意：\n",
    "和上面学习率一样，L1_reg_val_acc的存储也需要和正则化参数值对应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "69F1B83300954853826A6383607380B2",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate = 1.5e-6\n",
    "reg_types = ['L1', 'L2']\n",
    "L1_reg_val_acc = []\n",
    "L2_reg_val_acc = []\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2)\n",
    "best_L1_model = None\n",
    "best_L2_model = None\n",
    "best_L1_reg = 0\n",
    "best_L2_reg = 0\n",
    "best_L1_acc = 0\n",
    "best_L2_acc = 0\n",
    "\n",
    "########################################\n",
    "# TODO: train & test model with different regularization \n",
    "\n",
    "\n",
    "########################################\n",
    "\n",
    "#visulize the relation of regularization parameter and validation accuracy\n",
    "x = range(len(regs))\n",
    "plt.plot(x, L1_reg_val_acc, label='L1_val_acc')\n",
    "plt.plot(x, L2_reg_val_acc, label='L2_val_acc')\n",
    "plt.xticks(x, regs)\n",
    "plt.margins(0.08)\n",
    "plt.legend()\n",
    "plt.xlabel('high-parameter reg')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.show()\n",
    "\n",
    "#Compute the performance of best model on the test set\n",
    "L1_pred = best_L1_model.predict(X_test)\n",
    "L1_acc = np.mean(L1_pred == Y_test)\n",
    "print(\"The Accuracy with L1 regularization parameter {} is {}\\n\".format(best_L1_reg, L1_acc))\n",
    "L2_pred = best_L2_model.predict(X_test)\n",
    "L2_acc = np.mean(L2_pred == Y_test)\n",
    "print(\"The Accuracy with L2 regularization parameter {} is {}\\n\".format(best_L2_reg, L2_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FE5B019BDA934812B195721468F03A19",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 二、多分类逻辑回归\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CDA14DC4417C433280B811EB721C78AB",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 2.1 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2435572FAB2F49589F97188F6F3E00F3",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def load_data2(path):\n",
    "    # load all MNIST data\n",
    "    fd = open(os.path.join(path, 'train-images.idx3-ubyte'))\n",
    "    loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
    "    train_X = loaded[16:].reshape((60000, 28, 28, 1)).astype(np.float)\n",
    "    fd = open(os.path.join(path, 'train-labels.idx1-ubyte'))\n",
    "    loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
    "    train_Y = loaded[8:].reshape(60000).astype(np.float)\n",
    "    fd = open(os.path.join(path, 't10k-images.idx3-ubyte'))\n",
    "    loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
    "    test_X = loaded[16:].reshape((10000, 28, 28, 1)).astype(np.float)\n",
    "    fd = open(os.path.join(path, 't10k-labels.idx1-ubyte'))\n",
    "    loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
    "    test_Y = loaded[8:].reshape(10000).astype(np.float)\n",
    "\n",
    "    #visualize data\n",
    "    sample_num = 8\n",
    "    num_classes = 10\n",
    "    for y in range(num_classes):\n",
    "        idxs = np.flatnonzero(train_Y == y)\n",
    "        idxs = np.random.choice(idxs, sample_num, replace=False)\n",
    "        for i, idx in enumerate(idxs):\n",
    "            plt_idx = i * num_classes + y + 1\n",
    "            plt.subplot(sample_num, num_classes, plt_idx)\n",
    "            plt.imshow(train_X[idx, :, :, :].reshape((28,28)),cmap=plt.cm.gray)\n",
    "            plt.axis('off')\n",
    "            if i == 0:\n",
    "                plt.title(y)\n",
    "    plt.show()\n",
    "\n",
    "    # reshaple into rows and normaliza\n",
    "    train_X = train_X.reshape((train_X.shape[0], -1))\n",
    "    test_X = test_X.reshape((test_X.shape[0], -1))\n",
    "    mean_image = np.mean(train_X, axis=0)\n",
    "    train_X = train_X - mean_image\n",
    "    test_X = test_X - mean_image\n",
    "\n",
    "    # add a bias columu into X\n",
    "    train_X = np.hstack([train_X, np.ones((train_X.shape[0], 1))])\n",
    "    test_X = np.hstack([test_X, np.ones((test_X.shape[0], 1))])\n",
    "    train_Y = train_Y.astype(np.int32)\n",
    "    test_Y = test_Y.astype(np.int32)\n",
    "    return train_X, train_Y, test_X, test_Y\n",
    "\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = load_data2('./dataset/Assignment1/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CF74B98EAA4D4C5A8141081AB68C18D8",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"X_train shape is {}\".format(X_train.shape))\n",
    "print(\"Y_train shape is {}\".format(Y_train.shape))\n",
    "print(\"X_test shape is {}\".format(X_test.shape))\n",
    "print(\"Y_test shape is {}\".format(Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CDE7A7DC8F9542EC93B98D91D6C0AA01",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 2.2逻辑回归模型\n",
    "在这一部分中你需要完成与二分类逻辑回归相同的任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "E876678D20034910A341F5EDC3715830",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    " from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "6A55B828E6AF46D88C07A902A8D51A51",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LinearRegression2(object):\n",
    "    def __init__(self):\n",
    "        self.W = None\n",
    "    def train(self, X, Y, learning_rate=1e-3, reg=1e-5, reg_type='L2', show_detail=True, num_iters=2000,\n",
    "              batch_size=128):\n",
    "        num_train, feat_dim = X.shape\n",
    "        num_classes = 10\n",
    "        self.W = 0.001 * np.random.randn(feat_dim, num_classes)\n",
    "        loss_history = []\n",
    "        if show_detail:\n",
    "            print(\"train begin\")\n",
    "        for i in range(num_iters):\n",
    "            batch_indices = np.random.choice(num_train, batch_size, replace=True)\n",
    "            X_batch = X[batch_indices]\n",
    "            Y_batch = Y[batch_indices]\n",
    "            if reg_type == 'L1':\n",
    "                loss, grad = self.l1_loss(X_batch, Y_batch, reg)\n",
    "            else:\n",
    "                loss, grad = self.l2_loss(X_batch, Y_batch, reg)\n",
    "            loss_history.append(loss)\n",
    "            \n",
    "            ########################################\n",
    "            # TODO: update W\n",
    "            \n",
    "            \n",
    "            ########################################\n",
    "       \n",
    "            if show_detail and i % 100 == 0:\n",
    "                print(\"In iteration {}/{} , the loss is {}\".format(i, num_iters, loss))\n",
    "        return loss_history\n",
    "\n",
    "    def calc(self, X, Y):\n",
    "        num_train, feat_dim = X.shape\n",
    "        class_dim = self.W.shape[1]\n",
    "        loss = 0\n",
    "        grad = np.zeros([class_dim, feat_dim])\n",
    "        #soft = softmax(np.dot(X, self.W))\n",
    "        for i in range(num_train):\n",
    "            soft = softmax(np.dot(X[i], self.W))\n",
    "            loss += -np.log(soft[Y[i]])\n",
    "            for j in range(class_dim):\n",
    "                prob = soft[j]\n",
    "                if j == Y[i]:\n",
    "                    prob -= 1\n",
    "                grad[j] += X[i] * prob\n",
    "        grad = grad.T\n",
    "        return loss / X.shape[0], grad / X.shape[0]\n",
    "\n",
    "    def l1_loss(self,X, Y, reg):\n",
    "        loss = 0\n",
    "        grad = None\n",
    "    \n",
    "        ########################################\n",
    "        # TODO: Calculate the loss and gradient\n",
    "        \n",
    "        \n",
    "        ########################################\n",
    "    \n",
    "        return loss, grad\n",
    "            \n",
    "    \n",
    "    def l2_loss(self, X, Y, reg):\n",
    "        loss = 0\n",
    "        grad = None\n",
    "        \n",
    "        ########################################\n",
    "        # TODO: Calculate the loss and gradient\n",
    "        \n",
    "        \n",
    "        ########################################\n",
    "        \n",
    "        return loss, grad\n",
    "\n",
    "    def predict(self, X):\n",
    "        Y_pred = None\n",
    "\n",
    "        ########################################\n",
    "        # TODO: Predict the label of X\n",
    "        \n",
    "        \n",
    "        ########################################\n",
    "    \n",
    "        return Y_pred\n",
    "        \n",
    "def divide(a, b):\n",
    "    return a / b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "357C61F2396B4616979B175BBDD0DE12",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 2.3 训练模型样例\n",
    "在这一部分，你不需要完成任何代码，你可以通过这一部分验证你上面实现的LogisticRegression2的代码是否实现正确。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BF68315494B84FB2B401C86CB8788A4A",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr_param = 1e-6\n",
    "reg_param = 0.01\n",
    "model = LinearRegression2()\n",
    "loss_history = model.train(X_train, Y_train, lr_param, reg_param, 'L2')\n",
    "pred = model.predict(X_test)\n",
    "acc = np.mean(pred == Y_test)\n",
    "print(\"The Accuracy is {}\\n\".format(acc))\n",
    "x = range(len(loss_history))\n",
    "plt.plot(x, loss_history, label='Loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Iteration Num')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "729A6A93708047EE8A92A46AEB87CCA7",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 2.4 学习率与损失函数、模型性能的关系\n",
    "因为学习率和正则化参数都是超参数，在一般的训练过程中，我们没办法直接优化，所以我们一般会将训练集细分成训练集和验证集，然后通过模型在验证集上的表现选择一个最优的超参数，再将它对应的最优的模型应用到测试集中。\n",
    "在这一部分你需要完成以下内容：\n",
    "1. 尝试多种不同的学习率\n",
    "2. 储存学习率对应的损失函数值到L1_loss和L2_loss中（我们对损失函数值进行了20步平均化处理）。\n",
    "3. 储存学习率对应的**在验证集上**的正确率到L1_lr_val_acc和L2_lr_val_acc中\n",
    "\n",
    "#### 注意：\n",
    "因为已有代码中L1_loss，L1_lr_val_acc都是数组，在可视化的过程中我们需要学习率和它们相对应，比如learning_rates[0]对应的loss和validation accuracy应该储存在数组index为0的位置\n",
    "\n",
    "#### 拓展：\n",
    "在这个部分中采取的损失函数都是定值，如果你有时间的话，可以尝试根据迭代轮数改变学习率，并比较不变的学习率和变化的学习率对于模型性能的影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5A6CB337C471493B8DE3A3F868E49186",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "reg = 0.01\n",
    "reg_types = ['L1', 'L2']\n",
    "L1_loss = []\n",
    "L2_loss = []\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2)\n",
    "L1_lr_val_acc = []\n",
    "L2_lr_val_acc = []\n",
    "\n",
    "########################################\n",
    "# TODO: train & test model with different learning rate\n",
    "\n",
    "\n",
    "########################################\n",
    "\n",
    "#visulize the relationship between lr and loss\n",
    "for i, lr in enumerate(learning_rates):\n",
    "    L1_loss_label = str(lr) + 'L1'\n",
    "    L2_loss_label = str(lr) + 'L2'\n",
    "    L1_loss_i = L1_loss[i]\n",
    "    L2_loss_i = L2_loss[i]\n",
    "    ave_L1_loss = np.zeros_like(L1_loss_i)\n",
    "    ave_L2_loss = np.zeros_like(L2_loss_i)\n",
    "    ave_step = 20\n",
    "    for j in range(len(L1_loss_i)):\n",
    "        if j < ave_step:\n",
    "            ave_L1_loss[j] = np.mean(L1_loss_i[0: j + 1])\n",
    "            ave_L2_loss[j] = np.mean(L2_loss_i[0: j + 1])\n",
    "        else:\n",
    "            ave_L1_loss[j] = np.mean(L1_loss_i[j - ave_step + 1: j + 1])    \n",
    "            ave_L2_loss[j] = np.mean(L2_loss_i[j - ave_step + 1: j + 1])\n",
    "    x = range(len(L1_loss_i))\n",
    "    plt.plot(x, ave_L1_loss, label=L1_loss_label)\n",
    "    plt.plot(x, ave_L2_loss, label=L2_loss_label)\n",
    "    \n",
    "plt.legend()\n",
    "plt.xlabel('high-parameter lr')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "#visulize the relationship between lr and accuracy\n",
    "x = range(len(learning_rates))\n",
    "plt.plot(x, L1_lr_val_acc, label='L1_val_acc')\n",
    "plt.plot(x, L2_lr_val_acc, label='L2_val_acc')\n",
    "plt.xticks(x, learning_rates)\n",
    "plt.margins(0.08)\n",
    "plt.legend()\n",
    "plt.xlabel('high-parameter lr')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4A4B01F04B1241E089EE29390FDE9D2D",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 2.5 正则项与模型性能\n",
    "在这一部分中，你需要完成以下内容：\n",
    "1. 尝试多个正则化参数的值\n",
    "2. 储存对应的在**验证集上**的正确率到L1_reg_val_acc和L2_reg_val_acc中\n",
    "3. 通过验证集X_val和Y_val选择最优的正则化超参数，并储存最优正则化参数和对应模型\n",
    "\n",
    "已有的代码会画出正则化参数和验证集上正确率的关系图，并计算最优的模型在测试集上的正确率。\n",
    "\n",
    "#### 注意：\n",
    "和上面学习率一样，L1_reg_val_acc的存储也需要和正则化参数值对应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A37EA8B9B3BE427EA3B674AFC033EB84",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate = 1.5e-5 #initial 1.5e-6\n",
    "reg_types = ['L1', 'L2']\n",
    "L1_reg_val_acc = []\n",
    "L2_reg_val_acc = []\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2)\n",
    "best_L1_model = None\n",
    "best_L2_model = None\n",
    "best_L1_reg = 0\n",
    "best_L2_reg = 0\n",
    "best_L1_acc = 0\n",
    "best_L2_acc = 0\n",
    "\n",
    "########################################\n",
    "# TODO: train & test model with different regularization \n",
    "\n",
    "\n",
    "########################################\n",
    "\n",
    "#visuliza the relation of regularization parameter and validation accuracy\n",
    "x = range(len(regs))\n",
    "plt.plot(x, L1_reg_val_acc, label='L1_val_acc')\n",
    "plt.plot(x, L2_reg_val_acc, label='L2_val_acc')\n",
    "plt.xticks(x, regs)\n",
    "plt.margins(0.08)\n",
    "plt.legend()\n",
    "plt.xlabel('high-parameter reg')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.show()\n",
    "\n",
    "#Compute the performance of best model on the test set\n",
    "L1_pred = best_L1_model.predict(X_test)\n",
    "L1_acc = np.mean(L1_pred == Y_test)\n",
    "print(\"The Accuracy with L1 regularization parameter {} is {}\\n\".format(best_L1_reg, L1_acc))\n",
    "L2_pred = best_L2_model.predict(X_test)\n",
    "L2_acc = np.mean(L2_pred == Y_test)\n",
    "print(\"The Accuracy with L1 regularization parameter {} is {}\\n\".format(best_L2_reg, L2_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EBA02A5D4B2147B583047207362FCAEC",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Question2: 对于上面的多分类逻辑回归模型，你觉得它的权重矩阵数值上会呈现出什么样子？你可以通过下面提供的可视化方法观察权重矩阵。\n",
    "\n",
    "#### Answer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8725006585094B9B99C1BBC765ED6312",
    "jupyter": {},
    "notebookId": "604b0f8989c874001528225d",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr_param = 1e-6\n",
    "reg_param = 1000\n",
    "model = LinearRegression2()\n",
    "loss_history = model.train(X_train, Y_train, lr_param, reg_param, 'L2')\n",
    "pred = model.predict(X_test)\n",
    "acc = np.mean(pred == Y_test)\n",
    "print(\"The Accuracy is {}\\n\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C562AA5DDE7841568E2687F5257F0E70",
    "jupyter": {},
    "notebookId": "604b0f8989c874001528225d",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    pic = np.delete((model.W.T)[i], -1)\n",
    "    pic = pic.reshape(28, -1)\n",
    "    pic_min = np.min(pic)\n",
    "    pic_max = np.max(pic)\n",
    "    pic = (pic - np.min(pic)) / (np.max(pic) - np.min(pic)) * 255.0\n",
    "    print(\"Weight matrix of digit \", i)\n",
    "    plt.imshow(pic, cmap = plt.cm.gray)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "851e4435b7baf3c1db238cbc54cd1615f457581506c5ad73c179a1347e93c0e0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
